{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLsFsXbPVkk9xNOZePQJX1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install spacy pyodbc"],"metadata":{"id":"_QiwIotmZz1w","executionInfo":{"status":"ok","timestamp":1712070749698,"user_tz":240,"elapsed":843,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1da89991-8e1f-4107-c918-748f8ee105fa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: pyodbc\n","Successfully installed pyodbc-5.1.0\n"]}]},{"cell_type":"code","source":["stop_words = [\n","    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\",\n","    \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\",\n","    \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\",\n","    \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\",\n","    \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\",\n","    \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\",\n","    \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\",\n","    \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\",\n","    \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n","    \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\",\n","    \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\",\n","    \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\",\n","    \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\",\n","    \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\",\n","    \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\",\n","    \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\",\n","    \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n","]\n"],"metadata":{"id":"i5XZc5zLXjrb","executionInfo":{"status":"ok","timestamp":1712070751173,"user_tz":240,"elapsed":316,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["sentence = u'Show all students with marks greater than 30'"],"metadata":{"id":"tu1yqsfoQRV_","executionInfo":{"status":"ok","timestamp":1712070752685,"user_tz":240,"elapsed":2,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# basic string replacements for count and sum\n","sentence = sentence.replace(\"total number\", \"count\")\n","sentence = sentence.replace(\"total\", \"sum\")\n","sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"r-JjGz9QQSSP","executionInfo":{"status":"ok","timestamp":1712070754271,"user_tz":240,"elapsed":238,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"472b0a05-a462-457c-82da-81dd5ea8103e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Show all students with marks greater than 30'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# remove the stop words\n","new_sentence = \"\"\n","for word in sentence.split():\n","    if word not in stop_words:\n","        new_sentence += word + \" \"\n","sentence = new_sentence.lstrip()\n","sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3qklCPBoQW6B","executionInfo":{"status":"ok","timestamp":1712070755760,"user_tz":240,"elapsed":146,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"0f385ffa-2671-4d58-fb32-6370557d8675"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Show students marks greater 30 '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["  #  for loaded_entity in db_model.loaded_entities:\n","        # for loaded_entity_value in loaded_entity[1]:\n","        #     if loaded_entity_value.lower() in sentence:\n","        #         sentence = replace_entities(sentence, loaded_entity_value, loaded_entity_value)\n","\n"],"metadata":{"id":"uFyx5jTEYVPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","\n","\n","class Matcher(object):\n","    def __init__(self):\n","        self.matcher = []\n","        return super().__init__()\n","\n","    def add(self, key, value):\n","        self.matcher.append((key, value))\n","\n","    def find(self, phrase):\n","        matches = []\n","        for match in self.matcher:\n","            if \" \" + str(match[1]) + \" \" in phrase \\\n","                    or phrase.startswith(str(match[1]) + \" \") \\\n","                    or phrase.endswith(\" \" + str(match[1])):\n","                matches.append(copy.copy(match))\n","        return matches"],"metadata":{"id":"ohZ-0OrrQdS2","executionInfo":{"status":"ok","timestamp":1712070757418,"user_tz":240,"elapsed":222,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"M-L0IK1jXg4z","executionInfo":{"status":"ok","timestamp":1712074431397,"user_tz":240,"elapsed":9238,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["doc = nlp(sentence)"],"metadata":{"id":"KTpNqWwMlntf","executionInfo":{"status":"ok","timestamp":1712074638438,"user_tz":240,"elapsed":284,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["identified_spans = []\n","identified_entities = []\n","\n","for chunk in doc.noun_chunks:\n","    identified_spans.append(chunk.text)\n","    # print(chunk.text, \" -- \", chunk.root.text, \" -- \", chunk.root.dep_, \" -- \", chunk.root.head.text)\n","for ent in doc.ents:\n","    identified_entities.append(ent.text)"],"metadata":{"id":"_lde9fAMlrKm","executionInfo":{"status":"ok","timestamp":1712074639049,"user_tz":240,"elapsed":1,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["identified_entities,identified_spans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2YGG8XOlu4X","executionInfo":{"status":"ok","timestamp":1712074640115,"user_tz":240,"elapsed":155,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"758dfaed-cb44-4c23-fa43-572fa655b3d1"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['30'], ['Show students'])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["lemma_exceptions = [\"greater\", \"less\", \"than\", \"more\"]\n","lemmatizedSentence = ''\n","for token in doc:\n","    lemmatizedSentence = lemmatizedSentence + (token.text if token.text in lemma_exceptions else token.lemma_) + \" \"\n","lemmatizedSentence = lemmatizedSentence.lstrip()"],"metadata":{"id":"FDzyvFTtl1yW","executionInfo":{"status":"ok","timestamp":1712074750692,"user_tz":240,"elapsed":154,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["matches = custom_matcher.find(lemmatizedSentence)\n","matched_entities = []\n","matched_columns = []\n","for match in matches:\n","    if match[0].endswith(\"TABLE\"):\n","        matched_entities.append(Entities(match[0].replace(\"_TABLE\",\"\")))\n","        lemmatizedSentence = replace_string(lemmatizedSentence, str(match[1]), match[0].replace(\"_TABLE\",\"\"))\n","    if match[0].endswith(\"COLUMN\"):\n","        columnType = [c.type_ for c in db_model.columns if c.name == match[0].replace(\"_COLUMN\",\"\").lower()]\n","        if len(columnType) > 0:\n","            columnType = columnType[0]\n","        matched_columns.append(Columns(match[0].replace(\"_COLUMN\",\"\"), columnType))\n","        lemmatizedSentence = replace_string(lemmatizedSentence, str(match[1]), match[0].replace(\"_COLUMN\",\"\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"26LoHp0Cl-QO","executionInfo":{"status":"error","timestamp":1712074785266,"user_tz":240,"elapsed":218,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"02908465-248f-4ae3-a5c0-3f18967e884a"},"execution_count":25,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'custom_matcher' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-fbfb80b0be9f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_matcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizedSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatched_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmatched_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TABLE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'custom_matcher' is not defined"]}]},{"cell_type":"code","source":["class Entities(object):\n","    def __init__(self, name=None, defaultColumn=None, primaryKey=None, isAverage=None, isMax=None, isMin=None, isCount=None, columns=None, condition=None, value_=None, isSum=None):\n","        self.name = name\n","        self.isAverage = isAverage\n","        self.isMax = isMax\n","        self.isMin = isMin\n","        self.isCount = isCount\n","        self.columns = []\n","        self.value_ = value_\n","        self.condition = condition\n","        self.defaultColumn = defaultColumn\n","        self.primaryKey = primaryKey\n","        self.isSum = isSum"],"metadata":{"id":"FQUYtY93X7Mh","executionInfo":{"status":"ok","timestamp":1712074785449,"user_tz":240,"elapsed":2,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["config_data = {\n","    \"sql\": {\n","        \"connection_string\":\"\"\n","    },\n","    \"phrase_splitter\": \" in |in | in| and |and | and| with |with | with| of |of | of\",\n","    \"default_columns\": {\n","        \"entities\": {\n","            \"student\":\"name\",\n","            \"subject\":\"name\",\n","            \"student_mark\":\"id\"\n","        }\n","    },\n","    \"entities_to_load\": [\n","        {\n","            \"entity\": \"subject\",\n","            \"column\": \"name\"\n","        },\n","        {\n","            \"entity\": \"student\",\n","            \"column\": \"name\"\n","        }\n","    ],\n","    \"synonyms\": {\n","        \"column\": [\n","            {\n","                \"original\": \"class\",\n","                \"synonyms\": [ \"standard\" ]\n","            }\n","        ],\n","        \"table\": [\n","            {\n","                \"original\": \"student\",\n","                \"synonyms\": [ \"children\", \"child\" ]\n","            }\n","        ]\n","    }\n","}"],"metadata":{"id":"GH7PfxwAYck3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","\n","\n","class Singleton(type):\n","    \"\"\"\n","    Define an Instance operation that lets clients access its unique\n","    instance.\n","    \"\"\"\n","\n","    def __init__(cls, name, bases, attrs, **kwargs):\n","        super().__init__(name, bases, attrs)\n","        cls._instance = None\n","\n","    def __call__(cls, *args, **kwargs):\n","        if cls._instance is None:\n","            cls._instance = super().__call__(*args, **kwargs)\n","        return cls._instance\n","\n","\n","class Configuration(metaclass=Singleton):\n","    def __init__(self):\n","        self.data = config_data\n","\n","    # sql starts\n","    def get_sql_connection_string(self):\n","        return self.data[\"sql\"][\"connection_string\"]\n","\n","    def get_tables_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '.', 'tables.sql'))) as query:\n","            return query.read()\n","\n","    def get_columns_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '.', 'columns.sql'))) as query:\n","            return query.read()\n","\n","    def get_FK_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '.', 'foreign_keys.sql'))) as query:\n","            return query.read()\n","\n","    def get_PK_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '.', 'primary_keys.sql'))) as query:\n","            return query.read()\n","\n","    def get_synonyms(self):\n","        return self.data[\"synonyms\"]\n","\n","    def get_phrase_splitter(self):\n","        return self.data[\"phrase_splitter\"]\n","\n","    def get_entitites_to_load(self):\n","        return self.data[\"entities_to_load\"]\n","\n","    # sql ends\n","\n","    def get_default_column(self, table_name):\n","        return self.data[\"default_columns\"][\"entities\"][table_name]"],"metadata":{"id":"VY64Qfq5bLRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Columns(object):\n","    def __init__(self, name=None, type_=None, isAverage=None, isMax=None, isMin=None, isCount=None, value_=None, condition=None, isSum=None):\n","        self.name = name\n","        self.type_ = type_\n","        self.isAverage = isAverage\n","        self.isMax = isMax\n","        self.isMin = isMin\n","        self.isCount = isCount\n","        self.value_ = value_\n","        self.condition = condition\n","        self.isSum = isSum"],"metadata":{"id":"1GWEZLcjY-X5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Relationship(object):\n","    def __init__(self, entity1, entity2, column1, column2):\n","        self.entity1 = entity1\n","        self.entity2 = entity2\n","        self.column1 = column1\n","        self.column2 = column2"],"metadata":{"id":"Tn7QoT3xZAfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Synonyms(object):\n","    def __init__(self, column, synonym):\n","        self.column = column\n","        self.synonym = synonym"],"metadata":{"id":"5qVD-ucPZE6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy.lemmatizer import Lemmatizer\n","from spacy.lookups import Lookups\n","import pyodbc"],"metadata":{"id":"GR5hLKaLZVJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy import displacy"],"metadata":{"id":"6m8CeUt3JGwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load the configuration\n","config = Configuration()"],"metadata":{"id":"uLCl8s_0Ziuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config.get_sql_connection_string()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ZvzKpE9mcQue","executionInfo":{"status":"ok","timestamp":1711937769608,"user_tz":240,"elapsed":5,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"207af32d-0652-4415-a6d2-84a2d18ce024"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["\n","# load the DB Model\n","db_model = DBModel()\n","# remove unneccesary words\n","stan_stop_words = [line.rstrip('\\n') for line in open(\"stopwords.txt\")]\n","# load spacy's english model\n","nlp = spacy.load('en_core_web_sm')\n","# exceptions\n","exceptions = [\"between\", \"more\", \"than\"]\n","lemma_exceptions = [\"greater\", \"less\", \"than\", \"more\"]"],"metadata":{"id":"NFHfd7FAa7lx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","\n","\n","class Singleton(type):\n","    \"\"\"\n","    Define an Instance operation that lets clients access its unique\n","    instance.\n","    \"\"\"\n","\n","    def __init__(cls, name, bases, attrs, **kwargs):\n","        super().__init__(name, bases, attrs)\n","        cls._instance = None\n","\n","    def __call__(cls, *args, **kwargs):\n","        if cls._instance is None:\n","            cls._instance = super().__call__(*args, **kwargs)\n","        return cls._instance\n","\n","\n","class Configuration(metaclass=Singleton):\n","    def __init__(self):\n","        self.data = config_data\n","\n","    # sql starts\n","    def get_sql_connection_string(self):\n","        return self.data[\"sql\"][\"connection_string\"]\n","\n","    def get_tables_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '..', 'models', 'sql_scripts', 'tables.sql'))) as query:\n","            return query.read()\n","\n","    def get_columns_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '..', 'models', 'sql_scripts', 'columns.sql'))) as query:\n","            return query.read()\n","\n","    def get_FK_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '..', 'models', 'sql_scripts', 'foreign_keys.sql'))) as query:\n","            return query.read()\n","\n","    def get_PK_sql_query(self):\n","        with open(os.path.abspath(\n","                os.path.join(os.path.dirname(__file__), '..', 'models', 'sql_scripts', 'primary_keys.sql'))) as query:\n","            return query.read()\n","\n","    def get_synonyms(self):\n","        return self.data[\"synonyms\"]\n","\n","    def get_phrase_splitter(self):\n","        return self.data[\"phrase_splitter\"]\n","\n","    def get_entitites_to_load(self):\n","        return self.data[\"entities_to_load\"]\n","\n","    # sql ends\n","\n","    def get_default_column(self, table_name):\n","        return self.data[\"default_columns\"][\"entities\"][table_name]"],"metadata":{"id":"FKOwe1KIYPad"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFJBsrL5W6lV"},"outputs":[],"source":["def process_sentence(sentence):\n","\n","    # basic string replacements for count and sum\n","    sentence = sentence.replace(\"total number\", \"count\")\n","    sentence = sentence.replace(\"total\", \"sum\")\n","\n","    # remove the stop words\n","    new_sentence = \"\"\n","    for word in sentence.split():\n","        if word not in stop_words:\n","            new_sentence += word + \" \"\n","    sentence = new_sentence.lstrip()\n","\n","    for loaded_entity in db_model.loaded_entities:\n","        for loaded_entity_value in loaded_entity[1]:\n","            if loaded_entity_value.lower() in sentence:\n","                sentence = replace_entities(sentence, loaded_entity_value, loaded_entity_value)\n","\n","    # run nlp on sentence\n","    doc = nlp(sentence)\n","\n","    identified_spans = []\n","    identified_entities = []\n","\n","    for chunk in doc.noun_chunks:\n","        identified_spans.append(chunk.text)\n","        # print(chunk.text, \" -- \", chunk.root.text, \" -- \", chunk.root.dep_, \" -- \", chunk.root.head.text)\n","    for ent in doc.ents:\n","        identified_entities.append(ent.text)\n","        # print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","\n","    # build the lemma sentence\n","    lemmatizedSentence = ''\n","    for token in doc:\n","        lemmatizedSentence = lemmatizedSentence + (token.text if token.text in lemma_exceptions else token.lemma_) + \" \"\n","    lemmatizedSentence = lemmatizedSentence.lstrip()\n","\n","    # stop word removal\n","    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n","\n","    # for chunk in docLemmatized.noun_chunks:\n","    #     print(chunk.text, chunk.root.text, chunk.root.dep_,\n","    #             chunk.root.head.text)\n","\n","    # get all tables and columns in the question\n","    # matches = matcher(docLemmatized)\n","    matches = custom_matcher.find(lemmatizedSentence)\n","    matched_entities = []\n","    matched_columns = []\n","    for match in matches:\n","        if match[0].endswith(\"TABLE\"):\n","            matched_entities.append(Entities(match[0].replace(\"_TABLE\",\"\")))\n","            lemmatizedSentence = replace_string(lemmatizedSentence, str(match[1]), match[0].replace(\"_TABLE\",\"\"))\n","        if match[0].endswith(\"COLUMN\"):\n","            columnType = [c.type_ for c in db_model.columns if c.name == match[0].replace(\"_COLUMN\",\"\").lower()]\n","            if len(columnType) > 0:\n","                columnType = columnType[0]\n","            matched_columns.append(Columns(match[0].replace(\"_COLUMN\",\"\"), columnType))\n","            lemmatizedSentence = replace_string(lemmatizedSentence, str(match[1]), match[0].replace(\"_COLUMN\",\"\"))\n","\n","    docLemmatized = nlp(lemmatizedSentence)\n","\n","    # print(\"####################\")\n","    # print(lemmatizedSentence)\n","    # print([(m[0], m[1]) for m in custom_matcher.matcher])\n","    # print([(m[0], m[1]) for m in matches])\n","    # print([m.name for m in matched_entities])\n","    # print([m.name for m in matched_columns])\n","    # print(\"####################\")\n","\n","    # get values for the captured columns in the above use case\n","    for token in docLemmatized:\n","\n","        # check of token matches any of the matched entities\n","        if token.text.upper() in [m.name for m in matched_entities]:\n","            matched_entity = next(me for me in matched_entities if me.name == token.text.upper())\n","\n","            contextual_span = get_neighbour_tokens(token)\n","            span_ranges = re.split(config.get_phrase_splitter(), contextual_span)\n","            for span in span_ranges:\n","                if matched_entity.name.lower() in span:\n","                    matched_entity.condition = \"=\"\n","                    if \"average\" in span:\n","                        matched_entity.isAverage = True\n","                    if \"avg\" in span:\n","                        matched_entity.isAverage = True\n","                    if \"maximum\" in span:\n","                        matched_entity.isMax = True\n","                    if \"max\" in span:\n","                        matched_entity.isMax = True\n","                    if \"minimum\" in span:\n","                        matched_entity.isMin = True\n","                    if \"min\" in span:\n","                        matched_entity.isMin = True\n","                    if \"count\" in span:\n","                        matched_entity.isCount = True\n","                    if \"sum\" in span:\n","                        matched_entity.isSum = True\n","                    if \"total\" in span:\n","                        matched_entity.isSum = True\n","\n","                    trimmed_span = span \\\n","                        .replace(\"average\", \"\") \\\n","                        .replace(\"maximum\", \"\") \\\n","                        .replace(\"minimum\", \"\") \\\n","                        .replace(\"greater than\", \"\") \\\n","                        .replace(\"less than\", \"\") \\\n","                        .replace(\"more than\", \"\") \\\n","                        .replace(\"min\", \"\") \\\n","                        .replace(\"max\", \"\") \\\n","                        .replace(\"count\", \"\") \\\n","                        .replace(\"sum\", \"\")\n","                    trimmed_span = ' '.join(trimmed_span.split())\n","                    doc_span = nlp(trimmed_span)\n","\n","                    for span_token in doc_span:\n","\n","                        if span_token.text.lower() == matched_entity.name.lower():\n","\n","                            if get_token_child_len(span_token) > 0:\n","                                span_token_child = next(itertools.islice(span_token.children, 1))\n","                                ent = next(en for en in db_model.entities if en.name.lower() == matched_entity.name.lower())\n","                                default_column = next(col for col in ent.columns if col.name.lower() == ent.defaultColumn.lower())\n","                                value = get_value(span_token_child.text, default_column.type_)\n","\n","                                identified_entity_exists = False\n","                                for identified_entity in identified_entities:\n","                                    if identified_entity in trimmed_span and str(value) in identified_entity:\n","                                        identified_entity_exists = True\n","                                        value = identified_entity\n","                                matched_entity.value_ = value\n","\n","\n","\n","            matched_entities = [me for me in matched_entities if me.name != token.text.upper()]\n","            matched_entities.append(matched_entity)\n","\n","\n","        # check of token matches any of the matched column\n","        if token.text.upper() in [m.name for m in matched_columns]:\n","            matched_column = next(mc for mc in matched_columns if mc.name == token.text.upper())\n","\n","            contextual_span = get_neighbour_tokens(token)\n","            span_ranges = re.split(config.get_phrase_splitter(), contextual_span)\n","            for span in span_ranges:\n","                # print(\"column : \", span)\n","                if matched_column.name.lower() in span:\n","                    matched_column.condition = \"=\"\n","                    if \"average\" in span:\n","                        matched_column.isAverage = True\n","                    if \"avg\" in span:\n","                        matched_column.isAverage = True\n","                    if \"maximum\" in span:\n","                        matched_column.isMax = True\n","                    if \"max\" in span:\n","                        matched_column.isMax = True\n","                    if \"minimum\" in span:\n","                        matched_column.isMin = True\n","                    if \"min\" in span:\n","                        matched_column.isMin = True\n","                    if \"greater than\" in span:\n","                        matched_column.condition = \">\"\n","                    if \"more than\" in span:\n","                        matched_column.condition = \">\"\n","                    if \"less than\" in span:\n","                        matched_column.condition = \"<\"\n","                    if \"count\" in span:\n","                        matched_column.isCount = True\n","                    if \"sum\" in span:\n","                        matched_column.isSum = True\n","                    if \"total\" in span:\n","                        matched_column.isSum = True\n","\n","                    trimmed_span = span \\\n","                        .replace(\"average\", \"\") \\\n","                        .replace(\"maximum\", \"\") \\\n","                        .replace(\"minimum\", \"\") \\\n","                        .replace(\"greater than\", \"\") \\\n","                        .replace(\"less than\", \"\") \\\n","                        .replace(\"more than\", \"\") \\\n","                        .replace(\"min\", \"\") \\\n","                        .replace(\"max\", \"\") \\\n","                        .replace(\"count\", \"\") \\\n","                        .replace(\"sum\", \"\")\n","                    trimmed_span = ' '.join(trimmed_span.split())\n","\n","                    doc_span = nlp(trimmed_span)\n","\n","                    for span_token in doc_span:\n","                        if span_token.text.lower() == matched_column.name.lower():\n","                            if get_token_child_len(span_token) > 0:\n","                                span_token_child = next(itertools.islice(span_token.children, 1))\n","                                value = get_value(span_token_child.text, matched_column.type_)\n","\n","                                identified_entity_exists = False\n","                                for identified_entity in identified_entities:\n","                                    if identified_entity in trimmed_span and str(value) in identified_entity and get_value(identified_entity, matched_column.type_) != \"NoValue\":\n","                                        identified_entity_exists = True\n","                                        value = identified_entity\n","                                matched_column.value_ = value\n","\n","\n","            matched_columns = [mc for mc in matched_columns if mc.name != token.text.upper()]\n","            matched_columns.append(matched_column)\n","\n","    for loaded_entity in db_model.loaded_entities:\n","        entity_name = loaded_entity[0]\n","        for loaded_entity_value in loaded_entity[1]:\n","            if loaded_entity_value.lower() in lemmatizedSentence.lower():\n","                if entity_name.lower() in [me.name.lower() for me in matched_entities]:\n","                    # already exists\n","                    # no In operator support as of now\n","                    print(\"entity already processed\")\n","                else:\n","                    en_def_col = next(col for en in db_model.entities if en.name.lower() == entity_name.lower() for col in en.columns if col.name.lower() == en.defaultColumn.lower())\n","                    if get_value(loaded_entity_value, en_def_col.type_) != \"NoValue\":\n","                        ent = Entities(entity_name.upper())\n","                        ent.condition = \"=\"\n","                        ent.value_ = get_value(loaded_entity_value, en_def_col.type_)\n","                        matched_entities.append(ent)\n","\n","    # final representation of columns (matched_columns) and entities (matched_entities), including max, min, average, conditions\n","    # now next is to build the SQL query generator\n","    # matched entities\n","    # print(\"####################\")\n","    # print(\"\\n\".join([(mc.name + \" -- \" + str(mc.value_) + \" -- \" + \" condition : \" + str(mc.condition) + \" -- \" + \" isMax : \" + str(mc.isMax) + \" -- \" + \" isMin : \" + str(mc.isMin) + \" -- \" + \" isAverage : \" + str(mc.isAverage) + \" -- \" + \" isSum : \" + str(mc.isSum) + \" -- \" + \" isCount : \" + str(mc.isCount)) for mc in matched_entities]))\n","    # print(\"####################\")\n","    # matched columns\n","    # print(\"\\n\".join([(mc.name + \" -- \" + str(mc.value_) + \" -- \" + \" condition : \" + str(mc.condition) + \" -- \" + \" isMax : \" + str(mc.isMax) + \" -- \" + \" isMin : \" + str(mc.isMin) + \" -- \" + \" isAverage : \" + str(mc.isAverage)) for mc in matched_columns]))\n","    # print(\"####################\")\n","\n","    sql_generator = SQLGenerator(matched_entities, matched_columns, db_model)\n","    print(\"=================================================================================\")\n","    result = sql_generator.get_sql()\n","    # print(sql_generator.query)\n","    # print(\"=================================================================================\")\n","    # print(result)\n","    # print(\"=================================================================================\")\n","    response = {}\n","    response['sql'] = sql_generator.query\n","    response['result'] = result[0]\n","    response['columns'] = result[1]\n","    return response\n"]},{"cell_type":"code","source":["# remove the stop words\n","new_sentence = \"\"\n","for word in sentence.split():\n","    if word not in stop_words:\n","        new_sentence += word + \" \"\n","sentence = new_sentence.lstrip()\n","sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"hYbZHAe1Jk0D","executionInfo":{"status":"ok","timestamp":1711983228824,"user_tz":240,"elapsed":146,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"e1bfa954-f6d0-4eca-9dbf-79902e134a98"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Show students marks greater 30 '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["loaded_entities = [('subject', ['Lokesh Lal', 'Lokesh', 'Lal', 'Munish Sharma', 'Munish', 'Sharma', 'Sumit Chharia', 'Sumit', 'Chharia', 'Manoj Gupta', 'Manoj', 'Gupta', 'Vishal Gupta', 'Vishal', 'Gupta', 'Ambika Shukla', 'Ambika', 'Shukla', 'Manoj Garg', 'Manoj', 'Garg', 'Gagandeep sampat', 'Gagandeep', 'sampat'])]\n"],"metadata":{"id":"H-4hmicgJ3Qp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z11d405FPsFq","executionInfo":{"status":"ok","timestamp":1711985016202,"user_tz":240,"elapsed":6794,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"c4bedb0c-18ca-431b-ad6e-f24d671f61f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.7.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n","  warnings.warn(warn_msg)\n"]}]},{"cell_type":"code","source":["doc = nlp(sentence)\n","identified_spans = []\n","identified_entities = []\n","\n","for chunk in doc.noun_chunks:\n","    identified_spans.append(chunk.text)\n","    print(chunk.text, \" -- \", chunk.root.text, \" -- \", chunk.root.dep_, \" -- \", chunk.root.head.text)\n","for ent in doc.ents:\n","    identified_entities.append(ent.text)\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccNwaPXwQhO8","executionInfo":{"status":"ok","timestamp":1711985143263,"user_tz":240,"elapsed":170,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"7eac388d-5e9f-44a0-ac78-c8e37ebe9ef2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Show students  --  students  --  nsubj  --  marks\n","30 28 30 CARDINAL\n"]}]},{"cell_type":"code","source":["exceptions = [\"between\", \"more\", \"than\"]\n","lemma_exceptions = [\"greater\", \"less\", \"than\", \"more\"]\n","\n","lemmatizedSentence = ''\n","for token in doc:\n","    lemmatizedSentence = lemmatizedSentence + (token.text if token.text in lemma_exceptions else token.lemma_) + \" \"\n","lemmatizedSentence = lemmatizedSentence.lstrip()\n","lemmatizedSentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VksMXLN3Q1Wv","executionInfo":{"status":"ok","timestamp":1711985287033,"user_tz":240,"elapsed":18,"user":{"displayName":"Sheel Taskar","userId":"03520117247921953869"}},"outputId":"1e484ae1-cfbf-4221-b060-43709cf8bfd4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'show student mark greater 30 '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"],"metadata":{"id":"g8zGjp1JRolB"},"execution_count":null,"outputs":[]}]}